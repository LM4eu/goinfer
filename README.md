# Goinfer

Inference api server for local ggml language models. Based on [Llama.cpp](https://github.com/ggerganov/llama.cpp)

- **Multi models**: switch between models at runtime
- **Inference queries**: http api and streaming response support
- **Tasks**: predefined language model tasks

<details>
<summary>:books: Read the <a href="https://synw.github.io/goinfer/">documentation</a></summary>

 - [Get started](https://synw.github.io/goinfer//get_started)
    - [Install](https://synw.github.io/goinfer//get_started/install)
    - [Configure](https://synw.github.io/goinfer//get_started/configure)
    - [Run](https://synw.github.io/goinfer//get_started/run)
 - [Api](https://synw.github.io/goinfer//api)
    - [Models state](https://synw.github.io/goinfer//api/models_state)
    - [Load model](https://synw.github.io/goinfer//api/load_model)
    - [Inference](https://synw.github.io/goinfer//api/inference)
    - [Tasks](https://synw.github.io/goinfer//api/tasks)

</details>