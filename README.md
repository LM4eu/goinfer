# Goinfer

Inference proxy server for local large language models (LLM) using `*.gguf` files.
Goinfer is based on [llama.cpp](https://github.com/ggml-org/llama.cpp) and [llama-swap](https://github.com/mostlygeek/llama-swap).

- **Multi models**: switch between models at runtime
- **Inference queries**: HTTP API and streaming response support
- **Admin web UI**: [Infergui](https://github.com/synw/infergui)
