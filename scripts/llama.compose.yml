# run:
#       podman compose -f llama.compose.yml up -d --build
#       docker compose -f llama.compose.yml up -d --build

services:

  llama:
    image: llama
    container_name: llama
    build:
      context: ../../llama.cpp
      dockerfile: ../goinfer/scripts/llama.Dockerfile
      args:
        - CUDA_VERSION=13.0     # versions to select the Nvidia container image
        - CUDA_PATCH=1          # see: https://hub.docker.com/r/nvidia/cuda
        - UBUNTU_VERSION=24.04
        - CUDA_DOCKER_ARCH=86   # https://developer.nvidia.com/cuda-gpus
        - UBUNTU_VERSION=22.04
    volumes:
      - ./ccache:/usr/share/ccache
    environment:
      - TZ=UTC0 # time.Unix() uses UTC instead of local time zone
      - CCACHE_DIR=/usr/share/ccache
      - CCACHE_MAXSIZE=5G
    # restart: unless-stopped
    expose:
      - 8080
