# Configure

Create a config file at the root

## Api server mode

Generate your `goinfer.ini` configuration file.

```bash
GI_MODELS_DIR=/path/to/my/models go run . -write
```

Use `GI_MODELS_DIR` to provide the root directory to your `*.gguf` models.
`goinfer` will also parse the sub-folders,
so you can keep organizing your models within a folders tree.

## Local mode with web frontend

Set the debug API key in order to use the local web frontend:

```bash
GI_MODELS_DIR=/path/to/my/models go run . -write -debug
```

### Example

```ini
# ‚ö†Ô∏è Set your API key, can be 64-hex-digit (32-byte) üö®
# Goinfer sets a random API key: ./goinfer -write
api_key = '0787066b85d2b186ffd826c7c083d8a5037e33a8aa2040ee6c330be01b540cbd'

# CORS whitelist (env. var: GI_ORIGINS)
origins = 'localhost'

# Goinfer recursively searches GGUF files in one or multiple folders separated by ':'
# List your GGUF dirs with: locate .gguf | sed -e 's,/[^/]*$,,' | uniq
# env. var: GI_MODELS_DIR
models_dir = '/home/me/path/to/models'

# The default model name to load at startup
# Can also be set with: ./goinfer -start <model-name>
default_model = ''

# List model names and their llama-server flags
[extra_models]
'ggml-org/Qwen2.5-Coder-1.5B-Q8_0-GGUF' = '--fim-qwen-1.5b-default'
'ggml-org/Qwen2.5-Coder-14B-Q8_0-GGUF+0.5B-draft' = '--fim-qwen-14b-spec'
'ggml-org/Qwen2.5-Coder-3B-Q8_0-GGUF' = '--fim-qwen-3b-default'
'ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF' = '--fim-qwen-7b-default'
'ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF+0.5B-draft' = '--fim-qwen-7b-spec'
'ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF' = '--fim-qwen-30b-default'
'ggml-org/bge-small-en-v1.5-Q8_0-GGUF' = '--embd-bge-small-en-default'
'ggml-org/e5-small-v2-Q8_0-GGUF' = '--embd-e5-small-en-default'
'ggml-org/gte-small-Q8_0-GGUF' = '--embd-gte-small-default'

[llama]
# path of llama-server
exe = '/home/me/llama.cpp/build/bin/llama-server'
# common args used for every model
common = '--props --no-warmup --no-mmap'
# extra args to let tools like Agent-Smith doing the templating (/completions endpoint)
goinfer = '--jinja --chat-template-file template.jinja'
# extra llama-server flag when ./goinfer is used without the -q flag
verbose = '--verbose-prompt'
# extra llama-server flag for ./goinfer -debug
debug = '--verbosity 3'

# Addresses (ports) to listen
# Address can be <ip|host>:<port> or simply :<port> when <host> is localhost
[listen]
':4444' = 'goinfer'    # /completions endpoint letting tools like Agent-Smith doing the templating
':5555' = 'llama-swap' # OpenAI-compatible API by llama-swap
```

### Parameters

- `models_dir` *string*: root directory of the `*.gguf` models (multiple folders separated by `:`)
- `api_key`: *string*: the API key to protect the server endpoints
- `origins` *[]string*: a list of authorized CORS URLs
- TODO: complete
