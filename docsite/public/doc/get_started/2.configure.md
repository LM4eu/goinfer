# Configure

Create a config file at the root

## Api server mode

Generate your `goinfer.yml` configuration file.

```bash
GI_MODELS_DIR=/path/to/my/models go run . -gen-gi-cfg
```

Use `GI_MODELS_DIR` to provide the root directory to your `*.gguf` models.
`goinfer` will also parse the sub-folders,
so you can keep organizing your models within a folders tree.

## Local mode with web frontend

Set the debug API keys in order to use the local web frontend:

```bash
GI_MODELS_DIR=/path/to/my/models go run . -gen-gi-cfg -debug
```

### Example

```yaml
# Configuration of https://github.com/LM4eu/goinfer

# Recursively search *.gguf files in one or multiple folders separated by ':'
models_dir: /home/me/my/models

server:
  api_key:
    # ‚ö†Ô∏è Set your private 64-hex-digit API keys (32 bytes) üö®
    admin: PLEASE SET SECURE API KEY
    user:  PLEASE SET SECURE API KEY
  origins: localhost
  listen:
    :5143: webui,models
    :2222: openai,goinfer
    :5555: llama-swap proxy

llama:
  exe: /home/me/llama.cpp/build/bin/llama-server
  args:
    # --props: enable /props endpoint to change global properties at runtime
    # --no-webui: do not start the Web UI HTTP server
    common: --props --no-webui --no-warmup
    goinfer: --jinja --chat-template-file template.jinja
```

### Parameters

- `models_dir` *string*: root directory of the `*.gguf` models (multiple folders separated by `:`)
- `server.api_key`: *map*: the API keys to protect the server endpoints
- `server.origins` *[]string*: a list of authorized CORS URLs
- TODO: complete
