# Configure

Create a config file at the root

## Api server mode

Generate your `goinfer.yml` configuration file.

```bash
GI_MODELS_DIR=/path/to/my/models go run . -write
```

Use `GI_MODELS_DIR` to provide the root directory to your `*.gguf` models.
`goinfer` will also parse the sub-folders,
so you can keep organizing your models within a folders tree.

## Local mode with web frontend

Set the debug API key in order to use the local web frontend:

```bash
GI_MODELS_DIR=/path/to/my/models go run . -write -debug
```

### Example

```yaml
# Goinfer recursively search GGUF files in one or multiple folders separated by ':'
# List your GGUF dirs with `locate .gguf | sed -e 's,/[^/]*$,,' | uniq`
models_dir: /home/me/models 

# ‚ö†Ô∏è Set your API key, can be 64‚Äëhex‚Äëdigit (32‚Äëbyte) üö®
# Generate these random API key with: ./goinfer -write
api_key: "PLEASE SET USER API KEY"
origins:   # CORS whitelist
  - "https://my‚Äëfrontend.example.com"
  - "http://localhost"
listen:
  # format:  <address>: <list of enabled services>
  # <address> can be <ip|host>:<port> or simply :<port> when <host> is localhost
  ":4444": goinfer     # /completions endpoint letting tools like Agent-Smith doing the templating
  ":5555": llama-swap  # OpenAI‚Äëcompatible API by llama‚Äëswap

llama:
  exe: /home/me/llama.cpp/build/bin/llama-server
  args:
    # common args used for every model
    common: --props --no-warmup --no-mmap
    # extra args to let tools like Agent-Smith doing the templating (/completions endpoint)
    goinfer: --jinja --chat-template-file template.jinja
    # extra llama-server flag when ./goinfer is used without the -q flag
    verbose: --verbose-prompt
    # extra llama-server flag for ./goinfer -debug
    debug: --verbosity 3
```

### Parameters

- `models_dir` *string*: root directory of the `*.gguf` models (multiple folders separated by `:`)
- `api_key`: *string*: the API key to protect the server endpoints
- `origins` *[]string*: a list of authorized CORS URLs
- TODO: complete
