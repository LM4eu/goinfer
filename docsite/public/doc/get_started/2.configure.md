# Configure

Create a config file at the root

## Api server mode

Generate your `goinfer.yml` configuration file.

```bash
GI_MODELS_DIR=/path/to/my/models go run . -gen-main-cfg
```

Use `GI_MODELS_DIR` to provide the root directory to your `*.gguf` models.
`goinfer` will also parse the sub-folders,
so you can keep organizing your models within a folders tree.

## Local mode with web frontend

Set the debug API keys in order to use the local web frontend:

```bash
GI_MODELS_DIR=/path/to/my/models go run . -gen-main-cfg -debug
```

### Example

```yaml
# Configuration of https://github.com/LM4eu/goinfer

# Goinfer recursively search GGUF files in one or multiple folders separated by ':'
# List your GGUF dirs with `locate .gguf | sed -e 's,/[^/]*$,,' | uniq`
models_dir: /home/me/models

server:
  api_key:
    # ‚ö†Ô∏è Set your API keys, can be 64‚Äëhex‚Äëdigit (32‚Äëbyte) üö®
    # Generate with `./goinfer -gen-main-cfg`
    admin: PLEASE SET SECURE API KEY
    user:  PLEASE SET SECURE API KEY
  origins: localhost
  listen:
    # format:  <address>: <comma‚Äëseparated list of enabled services>
    # <address> can be <ip|host>:<port> or simply :<port> when <host> is localhost
    :2222: openai goinfer models
    :5555: llama-swap

llama:
  exe: /home/me/llama.cpp/build/bin/llama-server
  args:
    # --props: enable /props endpoint to change global properties at runtime
    common: --props --no-warmup
    goinfer: --jinja --chat-template-file template.jinja
```

### Parameters

- `models_dir` *string*: root directory of the `*.gguf` models (multiple folders separated by `:`)
- `server.api_key`: *map*: the API keys to protect the server endpoints
- `server.origins` *[]string*: a list of authorized CORS URLs
- TODO: complete
