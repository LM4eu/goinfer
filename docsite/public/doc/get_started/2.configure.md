# Configure

Create a config file at the root

## Api server mode

Generate your `goinfer.ini` configuration file.

```bash
GI_MODELS_DIR=/path/to/my/models go run . -overwrite-all
```

Use `GI_MODELS_DIR` to provide the root directory to your `*.gguf` models.
`goinfer` will also parse the sub-folders,
so you can keep organizing your models within a folders tree.

## Local mode with web frontend

Set the debug API key in order to use the local web frontend:

```bash
GI_MODELS_DIR=/path/to/my/models go run . -overwrite-all -debug
```

### Example

```ini
# ‚ö†Ô∏è Set your API key, can be 64-hex-digit (32-byte) üö®
# Goinfer sets a random API key with: ./goinfer -overwrite-all
api_key = '166a7c4bb8e9da0e1c414049c20797ec0fb9053d6bb553bf3f2dfcf1183451f5'
# 
# CORS whitelist (env. var: GI_ORIGINS)
origins = 'localhost'
# 
# Goinfer recursively searches GGUF files in one or multiple folders separated by ':'
# List your GGUF dirs with: locate .gguf | sed -e 's,/[^/]*$,,' | uniq
# env. var: GI_MODELS_DIR
models_dir = '/home/me/path/to/models'
# 
# The default model name to load at startup
# Can also be set with: ./goinfer -start <model-name>
default_model = ''

# Download models using llama-server flags
# see : github.com/ggml-org/llama.cpp/blob/master/common/arg.cpp#L3000
[extra_models]
'OuteAI/OuteTTS-0.2-500M-GGUF+ggml-org/WavTokenizer' = '--tts-oute-default'
'ggml-org/Qwen2.5-Coder-1.5B-Q8_0-GGUF' = '--fim-qwen-1.5b-default'
'ggml-org/Qwen2.5-Coder-14B-Q8_0-GGUF+0.5B-draft' = '--fim-qwen-14b-spec'
'ggml-org/Qwen2.5-Coder-3B-Q8_0-GGUF' = '--fim-qwen-3b-default'
'ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF' = '--fim-qwen-7b-default'
'ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF+0.5B-draft' = '--fim-qwen-7b-spec'
'ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF' = '--fim-qwen-30b-default'
'ggml-org/embeddinggemma-300M-qat-q4_0-GGUF' = '--embd-gemma-default'
'ggml-org/gemma-3-12b-it-qat-GGUF' = '--vision-gemma-12b-default'
'ggml-org/gemma-3-4b-it-qat-GGUF' = '--vision-gemma-4b-default'
'ggml-org/gpt-oss-120b-GGUF' = '--gpt-oss-120b-default'
'ggml-org/gpt-oss-20b-GGUF' = '--gpt-oss-20b-default'

[llama]
# path of llama-server
exe = '/home/me/llama.cpp/build/bin/llama-server'
# common args used for every model
common = '--props --no-warmup --no-mmap'
# extra args to let tools like Agent-Smith doing the templating (/completions endpoint)
smith = '--jinja --chat-template-file template.jinja'
# extra llama-server flag when ./goinfer is used without the -q flag
verbose = '--verbose-prompt'
# extra llama-server flag for ./goinfer -debug
debug = '--verbosity 3'
# address can be 'host:port' or 'ip:por' or simply ':port' (for host = localhost)
addr = ':8080' # OpenAI-compatible API
```

### Parameters

- `models_dir` *string*: root directory of the `*.gguf` models (multiple folders separated by `:`)
- `api_key`: *string*: the API key to protect the server endpoints
- `origins` *[]string*: a list of authorized CORS URLs
- TODO: complete
