# Run

## Binary

### Local mode with gui

```bash
./goinfer -local
```

Open `http://localhost:5143` to have the gui

### Api server mode

```bash
./goinfer
```

## From source

```bash
go run .
```

## Options

### Verbosity

- `-q` (quiet disables the verbose output)
- `-debug`

### Localhost

Run in local without checking the API keys using the `-no-api-key` flag.

## Container

See: [`Containerfile`](https://github.com/LM4eu/goinfer/blob/main/Containerfile).

Build the container image using `docker`, `podman` or `buildah`:

```bash
docker  build -t goinfer .
podman  build -t goinfer .
buildah build -t goinfer .
```

Run the container using `docker` or `podman`:

```bash
docker run --rm -p 5143:5143 -v $PWD/goinfer.yml:/goinfer.yml -v $PWD/models:/models goinfer
podman run --rm -p 5143:5143 -v $PWD/goinfer.yml:/goinfer.yml -v $PWD/models:/models goinfer
```

The [`compose.yml`](https://github.com/LM4eu/goinfer/blob/main/compose.yml)
simplifies both, building and running the container:

```bash
docker-compose up
```

‚ö†Ô∏è In `goinfer.yml` set a different `api_key` for security purpose.
The `models_dir` is inside the container. Bind it to the host directory.

See the following `goinfer.yml` example:

```yaml
# Configuration of https://github.com/LM4eu/goinfer

# Goinfer recursively search GGUF files in one or multiple folders separated by ':'
# List your GGUF dirs with `locate .gguf | sed -e 's,/[^/]*$,,' | uniq`
models_dir: /home/me/models

server:
  api_key:
    # ‚ö†Ô∏è Set your API keys, can be 64‚Äëhex‚Äëdigit (32‚Äëbyte) üö®
    # Generate with `./goinfer -gen-main-cfg`
    admin: PLEASE SET SECURE API KEY
    user:  PLEASE SET SECURE API KEY
  origins: localhost
  listen:
    # format:  <address>: <comma‚Äëseparated list of enabled services>
    # <address> can be <ip|host>:<port> or simply :<port> when <host> is localhost
    :2222: openai goinfer models
    :5555: llama-swap

llama:
  exe: /home/me/llama.cpp/build/bin/llama-server
  args:
    # --props: enable /props endpoint to change global properties at runtime
    common: --props --no-warmup
    goinfer: --jinja --chat-template-file template.jinja
```

When using `docker-compose`,
you can set `GI_MODELS_DIR` in an environment variable
in a local `.env` file.
The following is an example for the `.env` file:

```env
GI_MODELS_DIR=/home/me/my/lm/models
SERVER_API_KEY=8a2f480d2c62216fdde5e06c88b0539d6a0f7030e051c434df321aafcfc7ff0d
```
