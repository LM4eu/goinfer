# Javascript API

Install the [@goinfer/api](https://github.com/LM4eu/goinfer-js/packages/api) library:

```bash
npm install @goinfer/api
```

## Doc andÂ example

[API doc](https://synw.github.io/goinfer-js/api/index.html)

[Typescript example usage](https://github.com/LM4eu/goinfer/blob/main/examples/js/template/goinfer_lib.ts)

## Usage

Initialize:

```ts
import { useGoinfer } from "@goinfer/api";

const api = useGoinfer({
  serverUrl: "http://localhost:4444",
  apiKey: "api_key",
});
```

With parameters to control streaming:

```ts
const goinfer = useGoinfer({
  serverUrl: "http://localhost:4444",
  apiKey: "api_key",
  onToken: (token) => {
    console.log("Received token:", token);
  },
  onStartEmit: (stats) => {
    console.log("Emission started:", stats);
  },
  onError: (errorMsg) => {
    console.error("Error encountered:", errorMsg);
  },
});
```

Types:

```ts
interface TempInferStats {
  thinkingTime: number;
  thinkingTimeFormat: string;
}

interface GoinferParams {
  serverUrl: string;
  apiKey: string;
  onToken?: (t: string) => void;
  onStartEmit?: (s: TempInferStats) => void;
  onError?: (e: string) => void;
}
```

### List of models

Get the current models from the server with info about the template type and context window size:

```ts
import type { Models } from "@goinfer/api";

const models: Models = await api.models();
```

Example response:

```json
{
  "count": 10,
  "models": {
    "Devstral-Small-2507-UD-Q5_K_XL": {
        "path": "/home/c/j3/agent-oli/models/Devstral-Small-2507-UD-Q5_K_XL.gguf",
        "size": 16788118752
    },
    "Qwen3-30B-A3B-Q4_K_M": {
        "path": "/home/c/j3/agent-oli/models/30b/Qwen3-30B-A3B-Q4_K_M.gguf",
        "size": 18556685824
    },
    "ggml-org/Qwen2.5-Coder-1.5B-Q8_0-GGUF": {
        "cmd": "/home/c/j3/llama.cpp/build/bin/llama-server --port 5869 --props --no-warmup --fim-qwen-1.5b-default",
    },
    "ggml-org/Qwen2.5-Coder-14B-Q8_0-GGUF": {
        "cmd": "/home/c/j3/llama.cpp/build/bin/llama-server --port 5870 --props --no-warmup --fim-qwen-14b-spec",
    },
    "ggml-org/Qwen2.5-Coder-3B-Q8_0-GGUF": {
        "cmd": "/home/c/j3/llama.cpp/build/bin/llama-server --port 5871 --props --no-warmup --fim-qwen-3b-default",
    },
    "ggml-org/Qwen2.5-Coder-3B-Q8_0-GGUF_qwen2.5-coder-3b-q8_0": {
        "path": "/home/c/.cache/llama.cpp/ggml-org_Qwen2.5-Coder-3B-Q8_0-GGUF_qwen2.5-coder-3b-q8_0.gguf",
        "size": 3285476160
    },
    "ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF": {
        "cmd": "/home/c/j3/llama.cpp/build/bin/llama-server --port 5873 --props --no-warmup --fim-qwen-7b-spec",
        "error": "file absent but configured in llama-swap.yml"
    },
    "ggml-vocab-bert-bge": {
        "path": "/home/c/j3/llama.cpp/models/ggml-vocab-bert-bge.gguf",
        "error": "two files have same model name (must be unique)",
        "size": 627549
    },
    "ollex/qwen2.5-coder_7b-instruct-q8_0_Q8_0": {
        "path": "/home/c/j3/agent-oli/models/ollex/qwen2.5-coder_7b-instruct-q8_0_Q8_0.gguf",
        "size": 8098525696
    }
  }
}
```

Types:

```ts
interface Models {
  count: number;
  error: string;
  models: Record<string, ModelInfo>;
}

interface ModelInfo {
  path: string;
  flags: string;
  error: string;
  size: number;
}

interface TemplateInfo {
  name: string;
  ctx: number;
  error: string;
}
```

### Inference

Run an inference query using the loaded model and default params:

```ts
const result = await api.infer("Prompt here");
console.log(result.text);
```

Run an inference query providing a model and some inference params:

```ts
const inferParams: InferQuery = {
  model: "Qwen3-30B-A3B-Q4_K_M",
  ctx: 1024,
  prompt: "Say hello in French",
  llama: {
    stream: true,
    temperature: 0.2,
    top_p: 0.35,
    max_tokens: 250,
  }
};
const result = await api.infer("Prompt here", inferenceParams);
console.log(result.text);
```

Types:

```ts
interface InferQuery {
  model?: string,
  ctx?: number,
  timeout?: number,
  template?: string,
  prompt: any;
  temperature?: float;
  dynatemp_range?: float;
  dynatemp_exponent?: float;
  top_k?: number;
  top_p?: float;
  min_p?: float;
  max_tokens?: number;
  n_indent?: number;
  n_keep?: number;
  stream?: bool;
  stop?: Array<string>;
  typical_p?: float;
  repeat_penalty?: float;
  repeat_last_n?: number;
  presence_penalty?: float;
  frequency_penalty?: float;
  dry_multiplier?: float;
  dry_base?: float;
  dry_allowed_length?: number;
  dry_penalty_last_n?: number;
  dry_sequence_breakers?: Array<char>;
  xtc_probability?: float;
  xtc_threshold?: float;
  mirostat?: bool;
  mirostat_tau?: float;
  mirostat_eta?: float;
  grammar?: string;
  json_schema?: map[string]any;
  seed?: number;
  ignore_eos?: bool;
  logit_bias?: Array<Array<any>>;
  n_probs?: number;
  min_keep?: number;
  t_max_predict_ms?: number;
  id_slot?: number;
  cache_prompt?: bool;
  return_tokens?: bool;
  samplers?: Array<string>;
  timings_per_token?: bool;
  return_progress?: bool;
  post_sampling_probs?: bool;
  response_fields?: Array<string>;
  lora?: []map[string]any;
}

interface TempInferStats {
  thinkingTime: number;
  thinkingTimeFormat: string;
}

interface InferResult extends TempInferStats {
  text: string;
  inferenceTime: number;
  emitTime: number;
  emitTimeFormat: string;
  totalTime: number;
  totalTimeFormat: string;
  tokensPerSecond: number;
  totalTokens: number;
}
```

### Aborting Inference

To halt any running inference process:

```ts
await api.abort();
```
