# Goinfer

Inference api server for local ggml language models. Based on [Llama.cpp](https://github.com/ggerganov/llama.cpp)

- **Multi models**: switch between models at runtime
- **Inference queries**: http api and websockets support
- **Tasks**: predefined language model tasks

## Quickstart

Download a binary from the releases section (Linux only)

## Local usage with a gui

Generate a config file, providing the absolute path to your model's directory,
where the .bin ggml models are stored:

```bash
./goinfer -localconf /absolute/path/to/my/models/directory
```

Create a tasks directory:

```bash
mkdir tasks
```

Run the server:

```bash
./goinfer -local
```

Open `http://localhost:5143` in a browser to get the gui

## Api server usage

Generate a config file, providing the absolute path to your model's directory,
where the .bin ggml models are stored:

```bash
./goinfer -conf /absolute/path/to/my/models/directory
```

Note: the api key in the config file has been autogenerated

Create a tasks directory, or edit the config file to provide a path to an existing one:

```bash
mkdir tasks
```

Run the server:

```bash
./goinfer
```

No gui is available, only the api