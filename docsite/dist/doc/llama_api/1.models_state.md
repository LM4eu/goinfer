# Models state

Get the current models state:

- `/model/state` *GET*: the current state of the models on the server

Example response:

```javascript
{
  "ctx": 1024,
  "isModelLoaded": false,
  "loadedModel": "",
  "models": {
    'Marx-3B-V2-Q4_1-GGUF.gguf': { name: 'human_response', ctx: 2048 },
    'WizardVicuna-Uncensored-3B-0719.gguf.q8_0.bin': { name: 'wizard_vicuna', ctx: 2048 },
    'dolphin-llama2-7b.gguf.q4_K_M.bin': { name: 'vicuna_system', ctx: 2048 },
    'llama-2-7b-chat-codeCherryPop.gguf.q4_K_M.bin': { name: 'alpaca', ctx: 4096 },
    'losslessmegacoder-llama2-7b-mini.Q4_K_M.gguf': { name: 'chatml', ctx: 4096 },
    'nous-hermes-llama-2-7b.q4_0.gguf': { name: 'unknown', ctx: 0 },
    'open-llama-7b-v2-open-instruct.gguf.q4_K_M.bin': { name: 'unknown', ctx: 0 },
    'orca-mini-3b.gguf.q8_0.bin': { name: 'orca', ctx: 2048 },
    'q5_1-gguf-mamba-gpt-3B_v4.gguf': { name: 'mamba', ctx: 2048 }
  }
}
```

Response properties:

- `isModelLoaded` *boolean*: is a model currently loaded in the server memory
- `loadedModel` *string*: the name of the currently loaded model, empty string if no model is loaded
- `models`: *Record<string, { name: string, ctx: number }>*: the available models in the models directory with info about context window size and template type. See <a href="javascript:openLink('/llama_api/templates')">the templates doc</a>
- `ctx`: *int*: the current size of the context window, in number of tokens (default *1024*)

## Example

```bash
curl http://localhost:5143/model/state  | python -m json.tool
```
