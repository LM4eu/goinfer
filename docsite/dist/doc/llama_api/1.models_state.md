# Models state

Get the current models state:

- `/model/state` *GET*: the current state of the models on the server

Example response:

```javascript
{
  "ctx": 1024,
  "isModelLoaded": false,
  "loadedModel": "",
  "models": [
    "orca-mini-3b.ggmlv3.q8_0",
    "tulu-7b.ggmlv3.q5_1",
    "orca-mini-7b.ggmlv3.q5_1",
    "open-llama-7B-open-instruct.ggmlv3.q5_1",
    "WizardLM-13B-1.0.ggmlv3.q4_0"
  ]
}
```

Response properties:

- `isModelLoaded` *boolean*: is a model currently loaded in the server memory
- `loadedModel` *string*: the name of the currently loaded model, empty string if no model is loaded
- `models`: *[]string*: a list of the available model names in the models directory
- `ctx`: *int*: the size of the context window, in number of tokens (default *1024*)

## Example

```bash
curl http://localhost:5143/model/state  | python -m json.tool
```

Output:
```javascript
{
    "ctx": 1024,
    "isModelLoaded": false,
    "loadedModel": "",
    "models": [
        "mamba-gpt-3b-v3.ggmlv3.q8_0",
        "orca-mini-3b.ggmlv3.q8_0",
        "llongma-2-7b.ggmlv3.q4_K_M",
        "llama-2-7b-chat-codeCherryPop.ggmlv3.q4_K_M",
        "orca-mini-v2_7b.ggmlv3.q4_K_M",
        "dolphin-llama2-7b.ggmlv3.q4_K_M",
        "nous-hermes-llama-2-7b.ggmlv3.q4_K_M",
        "open-llama-7B-open-instruct.ggmlv3.q5_1"
    ]
}
```