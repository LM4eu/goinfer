// Copyright 2025 The contributors of Goinfer.
// This file is part of Goinfer, a LLM proxy under the MIT License.
// SPDX-License-Identifier: MIT

package conf

import (
	"io"
	"log/slog"
	"math"
	"os"
	"reflect"
	"strings"

	"github.com/goccy/go-yaml"
	"github.com/lynxai-team/garcon/gerr"
	"github.com/lynxai-team/goinfer/proxy/config"
)

// LlamaSwapYML is the llama-swap config filename.
const (
	LlamaSwapYML    = "llama-swap.yml"
	useModelPresets = true
)

// ReadSwapFromReader uses the LoadConfigFromReader() from llama-swap project.
func (cfg *Cfg) ReadSwapFromReader(r io.Reader) error {
	swap, err := config.LoadConfigFromReader(r)
	if err != nil {
		slog.Error("Cannot load llama-swap config", "file", LlamaSwapYML, "err", err)
		os.Exit(1)
	}
	cfg.Swap = swap
	return cfg.ValidateSwap()
}

// WriteLlamaSwapYML generates the llama-swap configuration.
func WriteLlamaSwapYML(yml []byte) (bool, error) {
	header := `# DO NOT EDIT - This file is generated by Goinfer.
# Doc:
# - https://github.com/lynxai-team/goinfer/?tab=readme-ov-file#llama-swapyml
# - https://github.com/mostlygeek/llama-swap/wiki/Configuration

`
	return writeWithHeader(LlamaSwapYML, header, yml)
}

// GenLlamaSwapYAML generates the llama-swap configuration.
func (cfg *Cfg) GenLlamaSwapYAML(verbose, debug bool) ([]byte, error) {
	if cfg.Swap == nil {
		cfg.Swap = &config.Config{}
	}

	switch {
	case debug:
		cfg.Swap.LogLevel = "debug"
	case verbose:
		cfg.Swap.LogLevel = "info"
	default:
		cfg.Swap.LogLevel = "warn"
	}

	// HealthCheckTimeout has some limitations:
	// - "llama-server -hf model-name" is nice to ease deployment, but may take one or two hours for very large models (200GB+)
	// - very large models (480B) need minutes to initialize their tensors
	// - startup time is different from runtime health check (TODO: different check during startup)
	cfg.Swap.HealthCheckTimeout = 300 // 5 minutes to initialize 480B model
	cfg.Swap.MetricsMaxInMemory = 500
	cfg.Swap.StartPort = 5800

	// set the macros
	commonArgs := " " + cfg.Llama.Common
	if verbose {
		commonArgs += " " + cfg.Llama.Verbose
	}
	if debug {
		commonArgs += " " + cfg.Llama.Debug
	}
	cfg.Swap.Macros = config.MacroList{
		// we cannot reuse a macro in another macro,
		// because the MacroList order is not guaranteed by the YAML marshaling
		// (the method MacroList.MarshalYAML writes a map)
		config.MacroEntry{Name: "cmd-fim", Value: cfg.Llama.Exe + commonArgs},
		config.MacroEntry{Name: "cmd-common", Value: cfg.Llama.Exe + commonArgs + " --port ${PORT}"},
		config.MacroEntry{Name: "cmd-goinfer", Value: cfg.Llama.Exe + commonArgs + " --port ${PORT} " + cfg.Llama.Goinfer},
	}

	if useModelPresets {
		cfg.setModelPresets()
	} else {
		cfg.setSwapModels()
	}

	err := cfg.ValidateSwap()
	if err != nil {
		return nil, err
	}

	yml, er := yaml.Marshal(&cfg.Swap)
	if er != nil {
		return nil, gerr.Wrap(er, gerr.ConfigErr, "failed to marshal the llama-swap config")
	}

	return yml, nil
}

// FixModelName returns a modified model name.
func (cfg *Cfg) FixModelName(modelName string) string {
	_, ok := cfg.Swap.Models[modelName]
	if ok {
		return modelName // valid
	}

	betterName, reason := cfg.selectModelName(modelName, false)

	if modelName != "" && modelName != betterName {
		slog.Info("change requested model (invalid)", "old", modelName, "new", betterName, "reason", reason+" the requested model")
	}

	return betterName
}

func (cfg *Cfg) fixDefaultModel() {
	cfg.setSwapModels()

	_, ok := cfg.Swap.Models[cfg.DefaultModel]
	if ok {
		return // valid
	}

	betterName, reason := cfg.selectModelName(cfg.DefaultModel, true)

	if cfg.DefaultModel != "" && cfg.DefaultModel != betterName {
		slog.Warn("change invalid default_model", "old", cfg.DefaultModel, "new", betterName, "reason", reason+" default_model")
	}

	cfg.DefaultModel = betterName
}

func (cfg *Cfg) selectModelName(model string, useSmallest bool) (betterName, reason string) {
	lowModel := strings.ToLower(model)
	lowPath := ""
	supName := ""    // the DefaultModel contains a model name
	subName := ""    // a model name contains the DefaultModel
	supname := ""    // same as supName but with a lowercase comparison
	subname := ""    // same as subName but with a lowercase comparison
	minName := model // the name of the smallest model
	minSize := int64(math.MaxInt64)
	for name, mi := range cfg.getInfo() {
		lowName := strings.ToLower(name)
		switch {
		case model == "": // skip the following strings.Contains checks
		case strings.Contains(mi.Path, model):
			slog.Info("replace path or filename by valid model name", "old", model, "new", name)
			return name, "use a model having a path/file containing the"
		case strings.Contains(strings.ToLower(mi.Path), lowModel):
			lowPath = name
		case strings.Contains(model, name): // this model name is a portion of the default_model
			subName = name
		case strings.Contains(name, model): // this model name contains the default_model
			supName = name
		case strings.Contains(lowModel, lowName): // same as above but in lower case
			subname = name
		case strings.Contains(lowName, lowModel):
			supname = name
		default:
		}
		if useSmallest && mi.Size < minSize {
			minSize = mi.Size
			minName = name
		}
	}
	// if the given model name is not related to a pathname or filename,
	// then select one by order of preference:
	switch {
	case lowPath != "":
		return lowPath, "use a model having a path/file insensitively containing the"
	case subName != "":
		return subName, "use a full model name containing the"
	case supName != "":
		return supName, "use a model name being a substring of"
	case subname != "":
		return subname, "use a full model name insensitively containing the"
	case supname != "":
		return supname, "use a model name being a insensitive substring of"
	default:
		return minName, "use the smallest model because no model related to"
	}
}

func (cfg *Cfg) setModelPresets() {
	if cfg.Swap == nil {
		cfg.Swap = &config.Config{}
	}
	if cfg.Swap.Models == nil {
		cfg.Swap.Models = make(map[string]*config.ModelConfig, 1)
	} else {
		clear(cfg.Swap.Models)
	}

	cfg.Swap.Models["use-models-preset"] = &config.ModelConfig{
		Cmd:           "${cmd-common} --models-preset " + ModelsINI,
		CheckEndpoint: "/health",
		Proxy:         "http://localhost:${PORT}",
	}

	// on startup, Goinfer automatically runs `llama-server --models-preset models.ini`
	cfg.Swap.Hooks.OnStartup.Preload = []string{"use-models-preset"}
}

func (cfg *Cfg) setSwapModels() {
	info := cfg.getInfo()

	if cfg.Swap == nil {
		cfg.Swap = &config.Config{}
	}
	if cfg.Swap.Models == nil {
		cfg.Swap.Models = make(map[string]*config.ModelConfig, 2*len(info)+9)
	}

	commonMC := config.ModelConfig{Proxy: "http://localhost:${PORT}", CheckEndpoint: "/health"}
	fimMC := commonMC
	goinferMC := commonMC
	fimMC.Proxy = "http://localhost:8012" // the flag --fim-qwen-xxxx sets port=8012
	goinferMC.Unlisted = true             // hide model in /v1/models and /upstream responses

	for model, flags := range cfg.ExtraModels {
		gi := true
		mc := &commonMC
		switch {
		case flags == "":
			flags = "-hf " + model
		case strings.HasPrefix(flags, "--embd-"):
			gi = false
		case strings.HasPrefix(flags, "--fim-"):
			mc = &fimMC
			gi = false
		default:
		}
		cfg.addModelCfg(model, "${cmd-common}", flags, mc)
		if gi {
			goinferMC.UseModelName = model // overrides the model name that is sent to /upstream server
			cfg.addModelCfg(A_+model, "${cmd-goinfer}", flags, &goinferMC)
		}
	}

	// For each model, set two model settings:
	// 1. for the OpenAI endpoints
	// 2. for the /completion endpoint (suffix +A)
	for model, mi := range info {
		goinferMC.UseModelName = model // overrides the model name that is sent to /upstream server
		flags := mi.Flags + " -m " + mi.Path
		cfg.addModelCfg(model, "${cmd-common}", flags, &commonMC)      // API for Cline, RooCode, RolePlay...
		cfg.addModelCfg(A_+model, "${cmd-goinfer}", flags, &goinferMC) // API for Agent-Smith...
	}

	// when Goinfer starts, llama-server is started with the DefaultModel
	if cfg.DefaultModel != "" {
		cfg.Swap.Hooks.OnStartup.Preload = []string{cfg.DefaultModel}
	}
}

// Add the model settings within the llama-swap configuration.
func (cfg *Cfg) addModelCfg(model, cmd, flags string, mc *config.ModelConfig) {
	nMC := *mc // copy content (do not use same pointer)
	nMC.Cmd = cmd
	if flags != "" {
		nMC.Cmd += " " + flags
		if strings.Contains(flags, " -hf ") {
			// -hf may download a model for a while
			// but /health check will stop it,
			// so better to disable /health check
			nMC.CheckEndpoint = "none"
		}
	}

	model = strings.Replace(model, "-GGUF", "", 1)
	old, ok := cfg.Swap.Models[model]
	if ok {
		merge(old, &nMC, flags)
	}
	cfg.Swap.Models[model] = &nMC
}

// Add the model settings within the llama-swap configuration.
func merge(old, mc *config.ModelConfig, newFlags string) {
	if reflect.DeepEqual(old.Cmd, mc.Cmd) {
		return // same values
	}

	if old.Cmd == mc.Cmd {
		slog.Debug("Overwrite but same", "cmd", old.Cmd)
		return
	}

	if newFlags == "" {
		slog.Debug("Empty flags => Skip new, keep", "old", old.Cmd)
		*mc = *old
		return
	}

	oH := strings.Contains(old.Cmd, " -hf ")
	nH := strings.Contains(newFlags, "-hf ")
	oM := strings.Contains(old.Cmd, " -m ")
	nM := strings.Contains(newFlags, "-m ")

	if oH && oM {
		slog.Warn("Overwrite. Cannot use both -hf and -m", "cmd", old.Cmd)
		return
	}
	if nH && nM {
		slog.Warn("Overwrite. Cannot use both -hf and -m", "flags", newFlags)
		*mc = *old
		return
	}
	if oH && nM {
		slog.Debug("Skip -hf (old) to use -m (new)", "old", old.Cmd, "new", mc.Cmd)
		return
	}
	if nH && oM {
		slog.Debug("Skip -hf (new) to use -m (old)", "old", old.Cmd, "new", mc.Cmd)
		*mc = *old
		return
	}

	slog.Info("Merge", "old", old.Cmd)
	slog.Info("Merge", "new", mc.Cmd)
	mc.Cmd = old.Cmd + " " + newFlags
	slog.Info("Merge", "both", mc.Cmd)
}
