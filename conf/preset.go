// Copyright 2025 The contributors of Goinfer.
// This file is part of Goinfer, a LLM proxy under the MIT License.
// SPDX-License-Identifier: MIT

package conf

import (
	"bytes"
	"maps"
	"slices"
	"strconv"
	"strings"
)

// ModelsINI is the llama.cpp config filename.
const ModelsINI = "models.ini"

// WriteModelsINI inserts the header and writes the llama.cpp configuration for `--models-preset ./models.ini`.
func WriteModelsINI(yml []byte) (bool, error) {
	header := `# DO NOT EDIT - This file is generated by Goinfer.
#
# llama.cpp configurations using Model Presets:
#
#     llama-server --models-preset ./models.ini
#
# Each section in this file defines a new preset.
# Keys within a section correspond to command-line arguments (without leading dashes).
# For example, the argument --n-gpu-layer 123 is written as n-gpu-layer = 123.
# Short argument forms (e.g., c, ngl) and environment variable names (e.g., LLAMA_ARG_N_GPU_LAYERS) are also supported as keys.
#
# Doc: https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md#model-presets
`
	return writeWithHeader(ModelsINI, header, yml)
}

// GenModelsINI generates the llama.cpp configuration for `--models-preset ./models.ini`.
func (cfg *Cfg) GenModelsINI() []byte {
	out := bytes.NewBufferString("\n" + "version = 1" + "\n")
	// For each model, set two model settings:
	// 1. for the OpenAI endpoints
	// 2. for the /completion endpoint (suffix +A)
	info := cfg.getInfo()
	for _, model := range slices.Sorted(maps.Keys(info)) {
		mi := info[model]

		cfg.genModel(out, model, mi, false)

		genComment(out, mi)

		if agentSmith {
			cfg.genModel(out, model, mi, true)
		}
	}

	return out.Bytes()
}

// Add the model settings within the llama-swap configuration.
func genComment(out *bytes.Buffer, mi *ModelInfo) {
	out.WriteString("# size = " + strconv.FormatInt(mi.Size, 10))
	if mi.Flags != "" {
		out.WriteString("\n" + "# args = " + mi.Flags)
	}
	if mi.Origin != "" {
		out.WriteString("\n" + "# args origin = " + mi.Origin)
	}
	if mi.Issue != "" {
		out.WriteString("\n" + "# issue = " + mi.Issue)
	}
}

// Add the model settings within the llama-swap configuration.
func (cfg *Cfg) genModel(out *bytes.Buffer, model string, mi *ModelInfo, as bool) {
	if as {
		model += plusA
	}

	out.WriteString(
		"\n" + "[" + model + "]" +
			"\n" + "model = " + mi.Path)

	if model == cfg.DefaultModel {
		out.WriteString("\n" + "load-on-startup = true")
	}

	var val string
	firstLoop := true
	if as {
		for arg := range strings.FieldsSeq(cfg.Llama.Smith) {
			val = genParam(out, val, arg, firstLoop)
			firstLoop = false
		}
	}
	for arg := range strings.FieldsSeq(mi.Flags) {
		val = genParam(out, val, arg, firstLoop)
		firstLoop = false
	}

	if !firstLoop {
		addValue(out, val)
	}

	out.WriteByte('\n')
}

// Add the model settings within the llama-swap configuration.
func genParam(out *bytes.Buffer, val, arg string, firstLoop bool) (newVal string) {
	// no leading dash => accumulate values
	if arg == "" || (arg[0] != '-') || len(arg) == 1 || ('0' <= arg[1] && arg[1] <= '9') {
		return val + " " + arg
	}

	if !firstLoop {
		addValue(out, val)
	}

	// new key: remove without the leading dash(es)
	i := 1
	if len(arg) > 2 && arg[1] == '-' {
		i = 2
	}
	newKey := arg[i:]

	out.WriteByte('\n')
	out.WriteString(newKey)
	out.WriteByte(' ')
	out.WriteByte('=')

	return ""
}

// Add the model settings within the llama-swap configuration.
func addValue(out *bytes.Buffer, val string) {
	switch {
	// manage the case of flags without values
	// --jinja => jinja = true
	case val == "":
		val = " true"
	// remove surrounding quotes
	case (val[1] == '"' && val[len(val)-1] == '"') ||
		(val[1] == '\'' && val[len(val)-1] == '\''):
		val = " " + val[2:len(val)-1]
	default:
	}
	out.WriteString(val)
}
