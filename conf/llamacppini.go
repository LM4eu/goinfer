// Copyright 2025 The contributors of Goinfer.
// This file is part of Goinfer, a LLM proxy under the MIT License.
// SPDX-License-Identifier: MIT

package conf

import (
	"bytes"
	"maps"
	"slices"
	"strconv"
	"strings"
)

// ModelsINI is the llama.cpp config filename.
const ModelsINI = "models.ini"

// WriteModelsINI insert the header and writes the llama.cpp configuration for `--models-preset ./models.ini`.
func WriteModelsINI(yml []byte) error {
	header := `# DO NOT EDIT - This file is generated by Goinfer.
#
# llama.cpp configurations using Model Presets:
#
#     llama-server --models-preset ./models.ini
#
# Each section in this file defines a new preset.
# Keys within a section correspond to command-line arguments (without leading dashes).
# For example, the argument --n-gpu-layer 123 is written as n-gpu-layer = 123.
# Short argument forms (e.g., c, ngl) and environment variable names (e.g., LLAMA_ARG_N_GPU_LAYERS) are also supported as keys.
#
# Doc: https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md#model-presets
`
	return writeWithHeader(ModelsINI, header, yml)
}

// GenModelsINI generates the llama.cpp configuration for `--models-preset ./models.ini`.
func (cfg *Cfg) GenModelsINI() []byte {
	out := bytes.NewBufferString(`
version = 1
`)
	// For each model, set two model settings:
	// 1. for the OpenAI endpoints
	// 2. for the /completion endpoint (suffix +A)
	info := cfg.getInfo()
	for _, model := range slices.Sorted(maps.Keys(info)) {
		mi := info[model]
		out.WriteString("\n# GGUF = " + mi.Path)
		out.WriteString("\n# size = " + strconv.FormatInt(mi.Size, 10))
		if mi.Flags != "" {
			out.WriteString("\n# args = " + mi.Flags)
		}
		if mi.Origin != "" {
			out.WriteString("\n# args origin = " + mi.Origin)
		}
		if mi.Error != "" {
			out.WriteString("\n# args origin = " + mi.Error)
		}

		genModel(out, model, mi.Path, mi.Flags)
		genModel(out, model+PLUS_A, mi.Path, cfg.Llama.Goinfer+" "+mi.Flags)
	}

	return out.Bytes()
}

// Add the model settings within the llama-swap configuration.
func genModel(out *bytes.Buffer, name, path, flags string) {
	out.WriteString(`
[` + name + `]
model = ` + path)

	var val string
	firstLoop := true
	for arg := range strings.FieldsSeq(flags) {
		val = genParam(out, val, arg, firstLoop)
		firstLoop = false
	}

	if !firstLoop {
		addValue(out, val)
	}

	out.WriteByte('\n')
}

// Add the model settings within the llama-swap configuration.
func genParam(out *bytes.Buffer, val, arg string, firstLoop bool) (newVal string) {
	// no leading dash => accumulate values
	if arg == "" || (arg[0] != '-') || len(arg) == 1 || ('0' <= arg[1] && arg[1] <= '9') {
		return val + " " + arg
	}

	if !firstLoop {
		addValue(out, val)
	}

	// new key: remove without the leading dash(es)
	i := 1
	if len(arg) > 2 && arg[1] == '-' {
		i = 2
	}
	newKey := arg[i:]

	out.WriteByte('\n')
	out.WriteString(newKey)
	out.WriteByte(' ')
	out.WriteByte('=')

	return ""
}

// Add the model settings within the llama-swap configuration.
func addValue(out *bytes.Buffer, val string) {
	switch {
	// manage the case of flags without values
	// --jinja => jinja = true
	case val == "":
		val = " true"
	// remove surrounding quotes
	case (val[1] == '"' && val[len(val)-1] == '"') ||
		(val[1] == '\'' && val[len(val)-1] == '\''):
		val = " " + val[2:len(val)-1]
	default:
	}
	out.WriteString(val)
}
